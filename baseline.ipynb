{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# custom\n",
    "import model\n",
    "import utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE = 'RoboticArm' # choose between BrushlessMotor and RoboticArm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the training and testing HDF5 dataset files\n",
    "TRAIN_DATASET_PATH = f'data/{MACHINE}/windowed/train_dataset_window_0.100s.h5'\n",
    "TEST_DATASET_PATH = f'data/{MACHINE}/windowed/test_dataset_window_0.100s.h5'\n",
    "\n",
    "# List of sensor names to be extracted from the dataset\n",
    "SENSORS = [\n",
    "    'imp23absu_mic',\n",
    "    'ism330dhcx_acc',\n",
    "    'ism330dhcx_gyro'\n",
    "]\n",
    "\n",
    "# List of label names to be extracted from the dataset\n",
    "LABEL_NAMES = ['segment_id',\n",
    "               'split_label',\n",
    "               'anomaly_label',\n",
    "               'domain_shift_op',\n",
    "               'domain_shift_env']\n",
    "\n",
    "PARAMS = {\n",
    "    'layer_dims': [2048, 2048, 2048, 16],\n",
    "    'lr': 0.0001,\n",
    "    'criterion': utilities.MSE,\n",
    "    'batch_size': 1024,\n",
    "    'num_epochs': 1000,\n",
    "    # TO BE ADAPTED TO YOUR MACHINE: either 'mps or 'cuda' if GPU available,\n",
    "    # otherwise 'cpu'\n",
    "    'device': 'cuda',\n",
    "    'patience': 3,\n",
    "    'normalisation': 'std_window',\n",
    "    'valid_size': 0.1,\n",
    "    'seed': 1995\n",
    "}\n",
    "\n",
    "# Set the seed for general torch operations\n",
    "torch.manual_seed(PARAMS['seed'])\n",
    "# Set the seed for MPS torch operations (ones that happen on the MPS Apple GPU)\n",
    "\n",
    "if PARAMS['device'] == 'mps':\n",
    "    torch.mps.manual_seed(PARAMS['seed'])\n",
    "elif PARAMS['device'] == 'cuda':\n",
    "    torch.cuda.manual_seed(PARAMS['seed'])\n",
    "elif PARAMS['device'] == 'cpu':\n",
    "    torch.manual_seed(PARAMS['seed'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported device type: {PARAMS['device']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X_train_raw, Y_train_raw, X_test, Y_test = utilities.load_dataset(\n",
    "    TRAIN_DATASET_PATH, TEST_DATASET_PATH, LABEL_NAMES, SENSORS)\n",
    "\n",
    "# Combine anomaly labels and domain shift labels to form a combined label\n",
    "Y_train_raw['combined_label'] = Y_train_raw['anomaly_label'] + \\\n",
    "    Y_train_raw['domain_shift_op'] + Y_train_raw['domain_shift_env']\n",
    "Y_test['combined_label'] = Y_test['anomaly_label'] + \\\n",
    "    Y_test['domain_shift_op'] + Y_test['domain_shift_env']\n",
    "\n",
    "# Split training data into training and validation sets, maintaining the\n",
    "# stratified distribution of the combined label\n",
    "train_indices, valid_indices, _, _ = train_test_split(\n",
    "    range(len(Y_train_raw)),\n",
    "    Y_train_raw,\n",
    "    stratify=Y_train_raw['combined_label'],\n",
    "    test_size=PARAMS['valid_size'],\n",
    "    random_state=PARAMS['seed']\n",
    ")\n",
    "\n",
    "# Select the training and validation data based on the indices\n",
    "X_train = [sensor_data[train_indices] for sensor_data in X_train_raw]\n",
    "X_valid = [sensor_data[valid_indices] for sensor_data in X_train_raw]\n",
    "Y_train = Y_train_raw.iloc[train_indices].reset_index(drop=True)\n",
    "Y_valid = Y_train_raw.iloc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "# Normalize the training, validation, and test datasets using the\n",
    "# specified normalization method\n",
    "X_train, X_valid, X_test = utilities.normalize_data(\n",
    "    X_train, X_valid, X_test, PARAMS['normalisation'])\n",
    "\n",
    "# Extract the number of channels and window lengths for each sensor\n",
    "NUM_CHANNELS = [x.shape[1] for x in X_train]\n",
    "WINDOW_LENGTHS = [x.shape[2] for x in X_train]\n",
    "\n",
    "X_train_tensor = [torch.from_numpy(x).to(PARAMS['device']) for x in X_train]\n",
    "X_valid_tensor = [torch.from_numpy(x).to(PARAMS['device']) for x in X_valid]\n",
    "X_test_tensor = [torch.from_numpy(x).to(PARAMS['device']) for x in X_test]\n",
    "\n",
    "train_dataset = utilities.CustomDataset(X_train_tensor)\n",
    "valid_dataset = utilities.CustomDataset(X_valid_tensor)\n",
    "test_dataset = utilities.CustomDataset(X_test_tensor)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset, batch_size=PARAMS['batch_size'], shuffle=True)\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset, batch_size=PARAMS['batch_size'], shuffle=False)\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset, batch_size=PARAMS['batch_size'], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = model.AutoencoderFC(WINDOW_LENGTHS, NUM_CHANNELS, PARAMS, SENSORS)\n",
    "optimizer = torch.optim.Adam(baseline.parameters(), lr=PARAMS['lr'])\n",
    "baseline.fit(train_data_loader, valid_data_loader, optimizer)\n",
    "AUC_scores = baseline.test(test_data_loader, Y_test, utilities.MSE, 'median')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results table as in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = AUC_scores.copy()\n",
    "results.columns = ['S+T', 'Source', 'Target']\n",
    "new_order = [\n",
    "    'total_loss',\n",
    "    'f_ism330dhcx_acc',\n",
    "    's_ism330dhcx_acc',\n",
    "    'f_ism330dhcx_gyro',\n",
    "    's_ism330dhcx_gyro',\n",
    "    'f_imp23absu_mic',\n",
    "    's_imp23absu_mic']\n",
    "results = results.reindex(new_order)\n",
    "results.index = [\n",
    "    'Overall',\n",
    "    'F-acc',\n",
    "    'S-acc',\n",
    "    'F-gyr',\n",
    "    'S-gyr',\n",
    "    'F-mic',\n",
    "    'S-mic']\n",
    "results = results * 100\n",
    "results = results.round(2)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MODEL_PATH = f'models/{MACHINE}'\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "torch.save(\n",
    "    baseline.state_dict(),\n",
    "    MODEL_PATH +\n",
    "    os.sep +\n",
    "    f'baseline_seed{PARAMS[\"seed\"]}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = f'results/{MACHINE}'\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "results.to_csv(RESULTS_PATH + os.sep + 'AUC_scores.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
