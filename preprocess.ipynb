{
          "cells": [
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# built-in\n",
                                        "import os\n",
                                        "from time import gmtime, strftime\n",
                                        "\n",
                                        "# libraries\n",
                                        "import h5py\n",
                                        "import numpy as np\n",
                                        "import pandas as pd\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Initializations\n",
                                        "MACHINE = 'RoboticArm'\n",
                                        "INPUT_FOLDER = f'data/{MACHINE}'\n",
                                        "OUTPUT_FOLDER = f'data/{MACHINE}/windowed'\n",
                                        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
                                        "\n",
                                        "# constants\n",
                                        "# Duration of initial data time affected by the gyroscope warm-up period\n",
                                        "GYROSCOPE_WARM_UP_TIME = pd.to_timedelta('35ms')\n",
                                        "WINDOW_SIZE_TS = pd.to_timedelta('100ms')\n",
                                        "\n",
                                        "# Initialize utility dictionary for preprocessing operations\n",
                                        "sensor_dict = {\n",
                                        "    'imp23absu_mic': {\n",
                                        "        'fs': 16000,\n",
                                        "        'number_of_channel': 1\n",
                                        "    },\n",
                                        "    'ism330dhcx_acc': {\n",
                                        "        'fs': 7063,  # Estimated sampling rate calculated by averaging time deltas across all files\n",
                                        "        'number_of_channel': 3\n",
                                        "    },\n",
                                        "    'ism330dhcx_gyro': {\n",
                                        "        'fs': 7063,  # Estimated sampling rate calculated by averaging time deltas across all files\n",
                                        "        'number_of_channel': 3\n",
                                        "    }\n",
                                        "}\n",
                                        "\n",
                                        "for sensor in sensor_dict.keys():\n",
                                        "    sensor = sensor_dict[sensor]\n",
                                        "    sensor['window_length'] = int(\n",
                                        "        sensor['fs'] * WINDOW_SIZE_TS.total_seconds())\n",
                                        "sensor_dict\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "normal_source_train = pd.read_csv(\n",
                                        "    f'{INPUT_FOLDER}/train/attributes_normal_source_train.csv',\n",
                                        "    index_col=0)\n",
                                        "normal_target_train = pd.read_csv(\n",
                                        "    f'{INPUT_FOLDER}/train/attributes_normal_target_train.csv',\n",
                                        "    index_col=0)\n",
                                        "normal_source_test = pd.read_csv(\n",
                                        "    f'{INPUT_FOLDER}/test/attributes_normal_source_test.csv',\n",
                                        "    index_col=0)\n",
                                        "anomaly_source_test = pd.read_csv(\n",
                                        "    f'{INPUT_FOLDER}/test/attributes_anomaly_source_test.csv',\n",
                                        "    index_col=0)\n",
                                        "normal_target_test = pd.read_csv(\n",
                                        "    f'{INPUT_FOLDER}/test/attributes_normal_target_test.csv',\n",
                                        "    index_col=0)\n",
                                        "anomaly_target_test = pd.read_csv(\n",
                                        "    f'{INPUT_FOLDER}/test/attributes_anomaly_target_test.csv',\n",
                                        "    index_col=0)\n",
                                        "\n",
                                        "Train_Metadata = pd.concat(\n",
                                        "    [normal_source_train, normal_target_train], axis=0).reset_index(drop=True)\n",
                                        "Test_Metadata = pd.concat([normal_source_test,\n",
                                        "                           anomaly_source_test,\n",
                                        "                           normal_target_test,\n",
                                        "                           anomaly_target_test],\n",
                                        "                          axis=0).reset_index(drop=True)\n",
                                        "\n",
                                        "# create segment id column\n",
                                        "Train_Metadata['segment_id'] = Train_Metadata['imp23absu_mic'].apply(\n",
                                        "    lambda x: x.replace('imp23absu_mic_', ''))\n",
                                        "Test_Metadata['segment_id'] = Test_Metadata['imp23absu_mic'].apply(\n",
                                        "    lambda x: x.replace('imp23absu_mic_', ''))\n",
                                        "\n",
                                        "# add customized path to each filepath in the Metadata dataframes\n",
                                        "for sensor in sensor_dict.keys():\n",
                                        "    Train_Metadata[sensor] = INPUT_FOLDER + '/train/' + Train_Metadata[sensor]\n",
                                        "    Test_Metadata[sensor] = INPUT_FOLDER + '/test/' + Test_Metadata[sensor]\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Loop through each dataset split type ('train' and 'test') with\n",
                                        "# corresponding metadata\n",
                                        "for split_type, metadata in zip(\n",
                                        "        ['train', 'test'], [Train_Metadata, Test_Metadata]):\n",
                                        "    # Define the save path for the HDF5 file\n",
                                        "    save_path = '{}/{}_dataset_window_{:.3f}s.h5'.format(\n",
                                        "        OUTPUT_FOLDER,\n",
                                        "        split_type,\n",
                                        "        WINDOW_SIZE_TS.total_seconds()\n",
                                        "    )\n",
                                        "\n",
                                        "    print(save_path)\n",
                                        "\n",
                                        "    # Open the HDF5 file in write mode\n",
                                        "    with h5py.File(save_path, 'w') as h5file:\n",
                                        "        # ================================================================ INIT\n",
                                        "        # Initialize datasets dictionary to store HDF5 datasets\n",
                                        "        datasets = {}\n",
                                        "\n",
                                        "        # Create datasets for each sensor defined in sensor_dict\n",
                                        "        for sensor in sensor_dict.keys():\n",
                                        "            window_length = sensor_dict[sensor]['window_length']\n",
                                        "            number_of_channel = sensor_dict[sensor]['number_of_channel']\n",
                                        "\n",
                                        "            # Create a dataset for each sensor with specified shape and\n",
                                        "            # chunking\n",
                                        "            datasets[sensor] = h5file.create_dataset(\n",
                                        "                sensor,\n",
                                        "                shape=(0, number_of_channel, window_length),\n",
                                        "                maxshape=(None, number_of_channel, window_length),\n",
                                        "                chunks=True\n",
                                        "            )\n",
                                        "\n",
                                        "        # Create additional datasets for segment ID and various labels\n",
                                        "\n",
                                        "        # dataset containing the index of corresponding segment\n",
                                        "        datasets['segment_id'] = h5file.create_dataset(\n",
                                        "            'segment_id',\n",
                                        "            shape=(0, 1),\n",
                                        "            maxshape=(None, 1),\n",
                                        "            chunks=True,\n",
                                        "            dtype=h5py.string_dtype(encoding='utf-8')\n",
                                        "        )\n",
                                        "\n",
                                        "        # dataset containing split labels\n",
                                        "        datasets['split_label'] = h5file.create_dataset(\n",
                                        "            'split_label',\n",
                                        "            shape=(0, 1),\n",
                                        "            maxshape=(None, 1),\n",
                                        "            chunks=True,\n",
                                        "            dtype=h5py.string_dtype(encoding='utf-8')\n",
                                        "        )\n",
                                        "\n",
                                        "        # dataset containing anomaly labels\n",
                                        "        datasets['anomaly_label'] = h5file.create_dataset(\n",
                                        "            'anomaly_label',\n",
                                        "            shape=(0, 1),\n",
                                        "            maxshape=(None, 1),\n",
                                        "            chunks=True,\n",
                                        "            dtype=h5py.string_dtype(encoding='utf-8')\n",
                                        "        )\n",
                                        "\n",
                                        "        # dataset containing operational domain shift labels\n",
                                        "        datasets['domain_shift_op'] = h5file.create_dataset(\n",
                                        "            'domain_shift_op',\n",
                                        "            shape=(0, 1),\n",
                                        "            maxshape=(None, 1),\n",
                                        "            chunks=True,\n",
                                        "            dtype=h5py.string_dtype(encoding='utf-8')\n",
                                        "        )\n",
                                        "\n",
                                        "        # dataset containing environmental domain shift labels\n",
                                        "        datasets['domain_shift_env'] = h5file.create_dataset(\n",
                                        "            'domain_shift_env',\n",
                                        "            shape=(0, 1),\n",
                                        "            maxshape=(None, 1),\n",
                                        "            chunks=True,\n",
                                        "            dtype=h5py.string_dtype(encoding='utf-8')\n",
                                        "        )\n",
                                        "\n",
                                        "        # ============================================  DATA SEGMENTATION INTO\n",
                                        "        # Every row of the Metadata represent the i-th segment of one specific recording:\n",
                                        "        # the same segment is recorded for all sensors, named in the same way and its path\n",
                                        "        # is linked in the appropriate column of the dataframe\n",
                                        "\n",
                                        "        # Iterate over all segments in the metadata\n",
                                        "        for file_index in range(len(metadata)):\n",
                                        "            try:\n",
                                        "                print(\n",
                                        "                    f'Completed: {file_index / (len(metadata)-1)*100:.2f}%',\n",
                                        "                    end='\\r')\n",
                                        "\n",
                                        "                # Load and process data for each sensor\n",
                                        "                for sensor in sensor_dict:\n",
                                        "                    sensor_df = pd.read_parquet(metadata[sensor][file_index])\n",
                                        "                    sensor_df['Time'] = pd.to_datetime(\n",
                                        "                        sensor_df['Time'], unit='s')\n",
                                        "                    sensor_df.set_index('Time', inplace=True)\n",
                                        "                    sensor_df.sort_index(inplace=True)\n",
                                        "\n",
                                        "                    sensor_dict[sensor]['data_raw'] = sensor_df\n",
                                        "                    sensor_dict[sensor]['max_ts'] = sensor_df.index[-1]\n",
                                        "                    sensor_dict[sensor]['min_ts'] = sensor_df.index[0]\n",
                                        "\n",
                                        "                # Determine the time range for the segment: makes sure that\n",
                                        "                # there is available data for all sensors\n",
                                        "                max_ts_list = [sensor_dict[sensor]['max_ts']\n",
                                        "                               for sensor in sensor_dict]\n",
                                        "                min_ts_list = [sensor_dict[sensor]['min_ts']\n",
                                        "                               for sensor in sensor_dict]\n",
                                        "\n",
                                        "                start_timestamp = max(\n",
                                        "                    sensor_dict['ism330dhcx_gyro']['min_ts'] +\n",
                                        "                    GYROSCOPE_WARM_UP_TIME,\n",
                                        "                    max(min_ts_list))\n",
                                        "                end_timestamp = min(max_ts_list)\n",
                                        "\n",
                                        "                # Extract labels for the segment\n",
                                        "                segment_id = metadata['segment_id'][file_index]\n",
                                        "                split_label = metadata['split_label'][file_index]\n",
                                        "                anomaly_label = metadata['anomaly_label'][file_index]\n",
                                        "                domain_shift_op = metadata['domain_shift_op'][file_index]\n",
                                        "                domain_shift_env = metadata['domain_shift_env'][file_index]\n",
                                        "\n",
                                        "                flag = 1\n",
                                        "                number_of_window = (\n",
                                        "                    end_timestamp - start_timestamp) // WINDOW_SIZE_TS\n",
                                        "\n",
                                        "                # Iterate over each sensor to process the data into windows\n",
                                        "                for sensor in sensor_dict:\n",
                                        "                    sensor_df = sensor_dict[sensor]['data_raw']\n",
                                        "                    num_points_per_window = sensor_dict[sensor]['window_length']\n",
                                        "                    num_channel = sensor_dict[sensor]['number_of_channel']\n",
                                        "\n",
                                        "                    # Iterate over each window in the segment\n",
                                        "                    for window_idx in range(number_of_window):\n",
                                        "                        start = start_timestamp + window_idx * WINDOW_SIZE_TS\n",
                                        "                        end = start + WINDOW_SIZE_TS\n",
                                        "                        sensor_df_window = sensor_df[start:end].values\n",
                                        "\n",
                                        "                        # Zero-pad or truncate the window to match the expected\n",
                                        "                        # length\n",
                                        "                        l = len(sensor_df_window)\n",
                                        "                        if l < num_points_per_window:\n",
                                        "                            pad_size = num_points_per_window - l\n",
                                        "                            padding = np.zeros((pad_size, num_channel))\n",
                                        "                            sensor_df_window = np.vstack(\n",
                                        "                                [sensor_df_window, padding])\n",
                                        "                        else:\n",
                                        "                            sensor_df_window = sensor_df_window[:num_points_per_window, :]\n",
                                        "\n",
                                        "                        # Resize and store the windowed data in the HDF5\n",
                                        "                        # dataset\n",
                                        "                        current_size = datasets[sensor].shape[0]\n",
                                        "                        datasets[sensor].resize(current_size + 1, axis=0)\n",
                                        "                        datasets[sensor][-1] = sensor_df_window.T\n",
                                        "\n",
                                        "                        if flag:\n",
                                        "                            current_size = datasets['segment_id'].shape[0]\n",
                                        "\n",
                                        "                            datasets['segment_id'].resize(\n",
                                        "                                current_size + 1, axis=0)\n",
                                        "                            datasets['segment_id'][-1] = segment_id\n",
                                        "\n",
                                        "                            datasets['split_label'].resize(\n",
                                        "                                current_size + 1, axis=0)\n",
                                        "                            datasets['split_label'][-1] = split_label\n",
                                        "\n",
                                        "                            datasets['anomaly_label'].resize(\n",
                                        "                                current_size + 1, axis=0)\n",
                                        "                            datasets['anomaly_label'][-1] = anomaly_label\n",
                                        "\n",
                                        "                            datasets['domain_shift_op'].resize(\n",
                                        "                                current_size + 1, axis=0)\n",
                                        "                            datasets['domain_shift_op'][-1] = domain_shift_op\n",
                                        "\n",
                                        "                            datasets['domain_shift_env'].resize(\n",
                                        "                                current_size + 1, axis=0)\n",
                                        "                            datasets['domain_shift_env'][-1] = domain_shift_env\n",
                                        "\n",
                                        "                    flag = 0\n",
                                        "            except Exception as e:\n",
                                        "                print('could not read file index {}'.format(file_index), e)\n"
                              ]
                    }
          ],
          "metadata": {
                    "kernelspec": {
                              "display_name": ".venv",
                              "language": "python",
                              "name": "python3"
                    },
                    "language_info": {
                              "codemirror_mode": {
                                        "name": "ipython",
                                        "version": 3
                              },
                              "file_extension": ".py",
                              "mimetype": "text/x-python",
                              "name": "python",
                              "nbconvert_exporter": "python",
                              "pygments_lexer": "ipython3",
                              "version": "3.10.11"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 2
}
